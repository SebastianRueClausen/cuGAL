2024-06-05 11:53:09,209 - e.ex - INFO - Running command 'main'
2024-06-05 11:53:09,212 - e.ex - INFO - Started run with ID "650"
init1
[[<function loadnx at 0x7f840cf41bc0>, [['data/ca-GrQc.txt']]]]
<function loadnx at 0x7f840cf41bc0> ['data/ca-GrQc.txt']
init2
C
##<1>##
4158 4158
##<1>##
4158 4158
C
##<1>##
4158 4158
##<1>##
4135 4158
C
##<1>##
4158 4158
##<1>##
4124 4158
C
##<1>##
4158 4158
##<1>##
4067 4158
C
##<1>##
4158 4158
##<1>##
4076 4158
C
##<1>##
4158 4158
##<1>##
3957 4158
2024-06-05 11:53:09,911 - e.main - INFO - randcheck: (0.8521051667032807, 0.899643698093842)
runs/650
2024-06-05 11:53:09,912 - e.run_exp - INFO - Graph:(1/1)
2024-06-05 11:53:09,912 - e.run_exp - INFO - Noise_level:(1/6)
2024-06-05 11:53:09,912 - e.run_exp - INFO - iteration:(1/1)
args:  {'iter': 15, 'simple': True, 'mu': 0.5}
Fugal
/home/andreas.hansen/Framework_GraphAlignment_CUGAL/algorithms/FUGAL/sinkhorn.py:177: UserWarning: An output with one or more elements was resized since it had shape [4158], which does not match the required output shape [1, 4158]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1711403378171/work/aten/src/ATen/native/Resize.cpp:28.)
  torch.matmul(u, K, out=KTu)
2024-06-05 12:13:48,658 - e.run_exp - INFO - 
[[[0.7816]]]
2024-06-05 12:13:48,658 - e.run_exp - INFO - Noise_level:(2/6)
2024-06-05 12:13:48,658 - e.run_exp - INFO - iteration:(1/1)
2024-06-05 12:13:48,809 - e.run_algs - WARNING - Disc. Target: 4135 < 4158
args:  {'iter': 15, 'simple': True, 'mu': 0.5}
Fugal
/home/andreas.hansen/Framework_GraphAlignment_CUGAL/algorithms/FUGAL/sinkhorn.py:177: UserWarning: An output with one or more elements was resized since it had shape [4158], which does not match the required output shape [1, 4158]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1711403378171/work/aten/src/ATen/native/Resize.cpp:28.)
  torch.matmul(u, K, out=KTu)
2024-06-05 12:34:31,630 - e.run_exp - INFO - 
[[[0.7443]]]
2024-06-05 12:34:31,631 - e.run_exp - INFO - Noise_level:(3/6)
2024-06-05 12:34:31,631 - e.run_exp - INFO - iteration:(1/1)
2024-06-05 12:34:31,784 - e.run_algs - WARNING - Disc. Target: 4124 < 4158
args:  {'iter': 15, 'simple': True, 'mu': 0.5}
Fugal
/home/andreas.hansen/Framework_GraphAlignment_CUGAL/algorithms/FUGAL/sinkhorn.py:177: UserWarning: An output with one or more elements was resized since it had shape [4158], which does not match the required output shape [1, 4158]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1711403378171/work/aten/src/ATen/native/Resize.cpp:28.)
  torch.matmul(u, K, out=KTu)
2024-06-05 12:55:16,503 - e.run_exp - INFO - 
[[[0.7167]]]
2024-06-05 12:55:16,503 - e.run_exp - INFO - Noise_level:(4/6)
2024-06-05 12:55:16,503 - e.run_exp - INFO - iteration:(1/1)
2024-06-05 12:55:16,657 - e.run_algs - WARNING - Disc. Target: 4067 < 4158
args:  {'iter': 15, 'simple': True, 'mu': 0.5}
Fugal
/home/andreas.hansen/Framework_GraphAlignment_CUGAL/algorithms/FUGAL/sinkhorn.py:177: UserWarning: An output with one or more elements was resized since it had shape [4158], which does not match the required output shape [1, 4158]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1711403378171/work/aten/src/ATen/native/Resize.cpp:28.)
  torch.matmul(u, K, out=KTu)
2024-06-05 13:15:59,794 - e.run_exp - INFO - 
[[[0.6662]]]
2024-06-05 13:15:59,794 - e.run_exp - INFO - Noise_level:(5/6)
2024-06-05 13:15:59,794 - e.run_exp - INFO - iteration:(1/1)
2024-06-05 13:15:59,817 - e.run_algs - WARNING - Disc. Target: 4076 < 4158
args:  {'iter': 15, 'simple': True, 'mu': 0.5}
Fugal
/home/andreas.hansen/Framework_GraphAlignment_CUGAL/algorithms/FUGAL/sinkhorn.py:177: UserWarning: An output with one or more elements was resized since it had shape [4158], which does not match the required output shape [1, 4158]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1711403378171/work/aten/src/ATen/native/Resize.cpp:28.)
  torch.matmul(u, K, out=KTu)
2024-06-05 13:36:45,540 - e.run_exp - INFO - 
[[[0.6217]]]
2024-06-05 13:36:45,540 - e.run_exp - INFO - Noise_level:(6/6)
2024-06-05 13:36:45,540 - e.run_exp - INFO - iteration:(1/1)
2024-06-05 13:36:45,563 - e.run_algs - WARNING - Disc. Target: 3957 < 4158
args:  {'iter': 15, 'simple': True, 'mu': 0.5}
Fugal
/home/andreas.hansen/Framework_GraphAlignment_CUGAL/algorithms/FUGAL/sinkhorn.py:177: UserWarning: An output with one or more elements was resized since it had shape [4158], which does not match the required output shape [1, 4158]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1711403378171/work/aten/src/ATen/native/Resize.cpp:28.)
  torch.matmul(u, K, out=KTu)
2024-06-05 13:57:38,880 - e.run_exp - INFO - 
[[[0.5707]]]
2024-06-05 13:57:38,881 - e.ex - INFO - Completed after 2:04:30
