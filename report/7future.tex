\section{Future Work}\label{future-work}

\subsection{LAP Solver}
Although our greedy LAP approximation gives good results in most cases, it struggles on certain graphs with high amounts of noise compared to an optimal solver. We expect that using GPU LAP solvers such as the CUDA implementation of the Hungarian algorithm by \cite{date2016gpu} or HyLAC \citep{kawtikwar2024hylac} will have similar runtime performance while increasing accuracy in high noise cases.

\subsection{Sinkhorn-Knopp}
The challenge of using Sinkhorn-Knopp with low-precision floating point representations remains a relatively unexplored area. We suspect that major improvements in accuracy and speed could be achieved from further research in the area. In addition, modifications to the Sinkhorn-Knopp algorithm such as by \cite{tang2024accelerating} could deliver further improvements.

\subsection{Memory Usage}
\textsc{cuGAL} shifts the main bottleneck of graph alignment from computation time towards memory usage. We expect that sparsity can be further utilised to reduce memory usage. Additionally, we expect that low-precision floats, such as 16-bit could be utilised in certain cases to take further advantage of modern GPUs optimised for deep learning. Memory limitations could also be improved by streaming data from RAM or even disk to the GPU as needed. Support for multiple GPUs could provide large improvements to both computation speed and memory limitations.

\subsection{Node Attributes}
We expect that \textsc{cuGAL} relatively easily could be extended to take advantage of node attributes by adding an additional LAP term to the objective function. This could extend the utility of \textsc{cuGAL} to problems where node attributes play an important role in determining a high quality alignment. 

\subsection{Further parameter tuning}
Although much of this project has focused on adding new parameters and tweaking these to better the results of both speed and accuracy, these parameters could be improved further. By conducting more tests on a wider array of datasets and noise types, we believe more optimal parameter values could be found, and perhaps determined automatically based on properties of the inputs graphs.